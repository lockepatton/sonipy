%%% template.tex
%%%
%%% This LaTeX source document can be used as the basis for your technical
%%% paper or abstract. Intentionally stripped of annotation, the parameters
%%% and commands should be adjusted for your particular paper - title, 
%%% author, article DOI, etc.
%%% The accompanying ``template.annotated.tex'' provides copious annotation
%%% for the commands and parameters found in the source document. (The code
%%% is identical in ``template.tex'' and ``template.annotated.tex.'')

\documentclass[]{acmsiggraph}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\TOGonlineid{45678}
\TOGvolume{0}
\TOGnumber{0}
\TOGarticleDOI{0}
\TOGprojectURL{}
\TOGvideoURL{}
\TOGdataURL{}
\TOGcodeURL{}
\usepackage{color}
%\definecolor{red}{rgb}{0.9, 0.17, 0.31}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{glsllst} % My own package providing markup listing for glsl
\usepackage{rmlst}   % My own package providing markup listing for renderman
\usepackage{amsmath}
\usepackage{hyperref}

\lstset{
	backgroundcolor=\color[rgb]{0.95, 0.95, 0.95},
	tabsize=3,
	%rulecolor=,
	basicstyle=\footnotesize\ttfamily,
	upquote=true,
	aboveskip={1.5\baselineskip},
	columns=fixed,
	showstringspaces=false,
	extendedchars=true,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	frame=none,
	aboveskip=15pt,
	belowskip=8pt,
	captionpos=t,
	showtabs=false,
	showspaces=false,
	showstringspaces=false,
	identifierstyle=\ttfamily,
	%keywordstyle=\color{red}\bfseries,
	%keywordstyle=[1]\bfseries\color{syntaxBlue},
	%keywordstyle=[2]\bfseries\color{syntaxRed},
	%keywordstyle=[3]\color{blue}\bfseries,
	%keywordstyle=[4]\bfseries\color{syntaxBlue},
	commentstyle=\color[rgb]{0.082,0.639,0.082},
	keywordstyle=[1]\bfseries\color[rgb]{0,0,0.75},
	keywordstyle=[2]\bfseries\color[rgb]{0.5,0.0,0.0},
	keywordstyle=[3]\bfseries\color[rgb]{0.127,0.427,0.514},
	keywordstyle=[4]\bfseries\color[rgb]{0.4,0.4,0.4},
	stringstyle=\color[rgb]{0.639,0.082,0.082},
}

\usepackage{listings}

% \setlength{\parindent}{4em}
\setlength{\parskip}{0.5em}

\title{\texttt{soni-py}: A Pitch-based Data Sonification Package}

\author{Locke Patton \textsuperscript{1,2}, Emily Levesque \textsuperscript{1}}
% \pdfauthor{Richard Southern}

\keywords{sonification}

\begin{document}

\teaser{
    \centering
    
    \vspace{-\baselineskip}
    \scriptsize  \textsuperscript{1}Department of Astronomy, University of Washington, Seattle, WA 98195 USA\\
    \textsuperscript{2}Harvard-Smithsonian Center for Astrophysics, 60 Garden St, Cambridge, MA 02138
    \normalsize

    \vspace{\baselineskip}
    \includegraphics[width=.65\linewidth]{paper/images/Picture1-nobkgd.png}
    \captionsetup{width=.65\linewidth}
    \caption{Example sonification case: an exploding star's change in brightness is plotted against time. Each data point corresponds to a tone blip at a frequency specified by its y value and a time specified by its x value. As the sound file plays, it scans the plot left to right, with the brightest moments of the exploding star reaching pitches of 3 times middle C on the piano and the tail of the cooling supernovae remnant dropping into lower audible pitches.}
    %Suggested rephrase: "...with the brightest peak of the supernova reaching a peak high frequency of 785 Hz (close to G5 on a piano) and the dimmest points of the fading supernova light curve corresponding to pitches of 70 Hz (roughly C$^#_2$ on a piano). The 216.63 Hz frequency of C4 (middle C) is marked for reference." [please check my guesstimated frequency numbers so they're exact or close to it!]
    %Note: Part of my rewrite above is using something called International Pitch Notation, a "technical" way to refer to notes/pitches. It's a little easiest for people who do think in pitches, because it's tempting to think of "3 times middle C" as "3 octaves above middle C", and so on. There's a nifty table that maps Hz to piano keys at https://pages.mtu.edu/~suits/notefreqs.html; we don't need to overuse this but if we DO want to mention musical notes this is the notation we can use! 
    %Note: supernova is singular, supernovae is plural! :P
    %Note: I think if we're being technically correct here the cooling is still the supernova, not the supernova remnant? We're still seeing light from the actual explosion and subsequent radioactive Ni/Co decay, rather than light from what we'd call a remnant.
}


\maketitle
\footnotetext[3]{locke.patton@cfa.harvard.edu}
\footnotetext[4]{emsque@uw.edu}

\section{Introduction} \label{sec:introduction}

\texttt{soni-py} moves beyond visual analyses by sonifying scatter-plot data, producing audio files that depict variations in y as perceptually uniform changes in pitch. Short tones - called blips - are sounded in time at intervals corresponding to x values.

The addition of this audio sonification to a scientist's toolset has the intention of creating more inclusive and accessible science, new attention grabbing public outreach opportunities, and an easier self-consistent scientific understanding of minor differences in data over large scales.

% The guarantees the user self consistency when translating from a visual medium to an audio medium. 

% The addition of audio light curves might allow for more inclusive science, viral public outreach, as well as a better scientific understanding of the minor differences in objects. - benjamin rose

% This specific case misses most of the examples in the JOSS documentation. However, I feel like this project could have a major impact on the field. In the study of odd astrophysical transients, a field that will see exponential growth in the next decade, light curve comparisons are the main scientific figure. This project has the potential to add audio “light curves” with fewer lines of code than you typically use to reformat a default matplotlib figure. The addition of audio light curves might allow for more inclusive science, viral public outreach, as well as a better scientific understanding of the minor differences in objects.

\subsection{Understanding pitch}
The cent is a logarithmic unit of measure for pitch intervals where $n \approx 3986\log(b/a)$ defines the number of cents between the pitch frequencies a and b.

\subsection{Human Pitch Sensitivity}
The average person is capable of discerning independent subsequent pitches with a difference of ~10 cents (Kollmeier et al. 2008). The human ear is most sensitive to frequencies between $\sim$500-4000 Hz, similar to the range of a standard piano.

With these parameters, xy scatterplot data can be translated into audio files that map y values to specific pitch frequencies, with the minimum discernible $\Delta y$ corresponding to a 10 cent pitch difference.

% \section{The Case for Sonification}

%\subsection{Why sonify lightcurves?}

% Thanks to the nature of human hearing, we can audibly discern subsequent pitch differences of 10 cents. On a y scale ranging from 0 to 10, that corresponds to hearing variations as small as $\Delta y\sim0.03$. This simultaneous depth and range makes pitch-varied audio an incredibly powerful and accessible tool for understanding nuances in data. Through our sonification efforts of astronomical data we have found that listeners can discern differences in the brightness, duration, and period of time-domain variations when the changes in brightness are expressed as changes in pitch.

%Furthermore, through our sonification efforts of periodically variable astronomy sources, we have generally found that periodicity can be discerned in our sonified data. For an example of this behavior in a sonification,  see the fast period eclipsing binary whose brightness is plotted against time in Figure \ref{ADDEXTRAEBFIG}.
%Note: This is *awesome* but let's be super-precise with the language. What we'd ideally like to be able to say is "there's periodicity there that our eye canNOT see but but our ears CAN hear, and computers prove us right". In short, we think we hear a period or pattern that we can't see, and computers then prove us right (because if we think we hear a period that isn't really there, that's not good!) Do we have a particular example of this? (if we don't that's actually fine; the paper doesn't rest on this claim! It's more that if we want to make it we want it to be solid)

% This approach also opens up science and citizen science to participants who are visually impaired, and empowers blind and visually impaired (BVI) individuals to explore their own data.

\section{Scientific Need}

This \texttt{soni-py} code is specifically designed to create an open-access scientifically useful method to listen to data, with accuracy and use on par with reading plots visually. While many sonification tools exist, this was specifically designed in collaboration with the University of Washington Speech and Hearing Sciences to guarantee that a linear increase in y value will correspond to a uniform increase in perceived “pitch”. This means that while frequency varies non linearly, the user is listening to data in a uniformly perceived way - a linear plot sounds like a linear sweep in pitch. The guarantees the user self consistency when translating from a visual medium to an audio medium. Furthermore, this technique allows us to probe smaller difference in y values than we can discern visually. We’ve also found that we can hear periodic details in data that are not easily visually discerned from a plot alone.

Thanks to the nature of human hearing, we can audibly discern subsequent pitch differences of 10 cents (a logarithmic measure of pitch interval). On a y scale ranging from 0 to 10, that corresponds to hearing variations as small as dy~0.03 - a number which rivals our visual perceptions of scatter plot detail. This simultaneous depth and range makes pitch-varied audio an incredibly powerful and accessible tool for understanding nuances in data. This approach also opens up science and citizen science to participants who are visually impaired, and empowers blind and visually impaired (BVI) individuals to explore their own data.

In fact, this code was specifically designed for this use case. Already it’s been used by students at Perkin’s School for the Blind, a semester Ohio State Astronomy Program for BVI high school students, and is the foundation for our NSF-funded TransientZoo project, a citizen science program that will allow participants, including BVI individuals, to classify astronomical supernova lightcurves using sound.

\section{State of the Field}

%  TO DO
% - incorporate citations?

Sonification of scientific data has a long history of precedent. The most relevant projects have been completed by SYSTEM Sounds, run by Matt Russo, with the intent of public astronomy outreach through sonifications. He and his team have used various sonic methods to produce auditory experiences of data, ranging from an exoplanet period correlated with musical beats to a scan across pictures of Saturn's ring matching image components to pitch and volume. Most similar to our work is his sonification of a Hubble image of a galaxy cluster - time flows from left to right across the image while the frequency of sound changes from bottom to top of the image; the brightness at any point correlates with loudness. Other works that sonify supernovae have been completed by Alicia Soderberg and Raffaella Margutti, matching different musical instrument sounds to different wavelength regimes and correlating the brightness of an object with pitch or loudness. In the field of astronomy more generally, the LIGO collaboration famously produces sound files mapping a 2D histogram of a black hole merger's gravitational wave signal to pitch and matching gravitational wave frequency to sound frequency (creating the distinctive upward ``chip" sound now associated with gravitational wave mergers).

However, with all of these sonification tools the purpose of the sound is to communicate science rather than to aurally analyze the data or increase research accessibility. Our code implements a sonification method that is perceptually consistent (using scientific measures of pitch perception rather than harder-to-discern variations such as loudness). This allows users to reliably analyze scatter plots by listening to them, making this the first sonification tool suitable for scientific research. Future data sonification codes in astronomy, such as \texttt{astronify}, are already building upon our core method, and potential applications in the field include collaborations with large survey projects in astronomy such as the Zwicky Transient Facility and the Vera C. Rubin Observatory.


% The data sonification project is led by the Chandra X-ray Center (CXC) as part of the NASA's Universe of Learning (UoL) program. NASA's Science Activation program strives to enable NASA science experts and to incorporate NASA science content into the learning environment effectively and efficiently for learners of all ages. The collaboration was driven by visualization scientist Kimberly Arcand (CXC), astrophysicist Matt Russo and musician Andrew Santaguida (both of the SYSTEMS Sound project.)




% The two codes that you linked above - @faroit - are both very interesting but distinct sonification cases. In emailing with @david-worrall, the author of the other code “sonipy”, it’s clear that their code is more an awesome cumulation of interface codes. While a potentially cool idea in the future to connect this sonipy with their infrastructure to connect more with the field of sonification, this code is most useful and accessible to BVI students as already designed and implemented. It’s a tool designed to be plugged into various science disciplines (in fields otherwise unfamiliar with sonification) and thus fits well without a complete sonification library.

% The second code interactive-sonification is awesome and designed for interactive audio processing. Our code is specifically designed non-interactively; while it uses audio processing, it uses thinkDSP as the audio processing tool. Our code does the work of designing a perceptually uniform sonification for accurate use in listening to science plots. We specially built this tool as there was no other sonification that filled this use.

\section{Our Sonification Technique}

As seen in Figure \ref{fig:method}, we built our technique so that each xy data point has a corresponding short tone called a blip. The y value of a given data point corresponds to the pitch of its blip, while the x value corresponds to the placement of the blip in time. More sampled x values have a great blip density in time, and as y value increases or decreases, the tone’s pitch gets higher or lower, respectively.

\begin{figure}
\centering
\includegraphics[width=.7\linewidth]{paper/images/Method1.png}
\caption{Each data point corresponds to a short tone or "blip" in the sound file. Here the x and y values of a sine function with some noise are shown in black. The x value of a given data point determines the placement of the tone in time. The y value determines the tone’s pitch. Beside each data point, we've placed a visualization of its blip, shown in color. This blip trail, with a length corresponding to the duration of the blip, shows the variation of the amplitude of the pitch at its frequency. Note that as the values get higher, the corresponding frequency of the blip increases greatly indicating a higher pitch. All of these blips are combined in time to create the sound file.}
\label{fig:method} 
\end{figure}

\subsection{Y Values: Pitch}

The y value of a given data point determines the tone’s pitch. A complete well-defined y frequency scale has the following parameters:

\begin{enumerate}
\item A minimum frequency $f_{min}$ and its corresponding minimum y value $y_{min}$
\item A maximum frequency $f_{max}$ and its corresponding maximum y value $y_{max}$
\item A change in pitch (measured in cents) over change in y value parameter $\frac{dc}{dy}$
\end{enumerate}

Fundamentally these values must be related via the following equation.
$$ f_{min} = \frac{f_{max}}{2^{\frac{dc}{dy} [y_{max} - y_{min}] ~/~ 1200}} $$

We then relate any given y value to its corresponding frequency $f$ via the following relationship.

$$ f = \frac{f_{max}}{2^{\frac{dc}{dy} [y_{max} - y] ~/~ 1200}} $$
Our code accepts either a maximum frequency and cent scale slope parameter or a maximum and minimum frequency to create a given frequency scale.

\subsection{X Values: Placement of Tones in Time}

The x value determines the placement of the tone in time. A complete well-defined x time scale has the following parameters:

\begin{enumerate}
\item A minimum x value $x_{min}$
\item A maximum x value $x_{max}$
\item A total time of the sound file $t_{total}$
\item A change in time (measured in seconds) over change in x value parameter $\frac{dt}{dx}$
\end{enumerate}

$$ t = \frac{dt}{dx} [x - x_{min}]$$
Our code accepts a total time, a smallest time difference between subsequent points, a largest time difference between subsequent points, or a value for the  $\frac{dt}{dx}$ parameter to create the time scale.

\subsection{Why this method? }

Each datapoint corresponds to a tone blip at a frequency specified by its y value and a time specified by its x value. As the sound file plays, it scans the plot left to right, with higher y datapoints causing higher-pitched blips and vice versa.

Our method is tailored to the capabilities of the human ear and audio equipment. It is flexible, applies to a broad variety of data inputs, is fast to generate, and offers a unique means of classifying data.

We avoid methods that match changes in y to decibels, because human perception of loudness is inconsistent across users and not a perceptually consistent space. As our method is tailored to a science case, a linear increase in y corresponds to a perceptually consistent and linear increase in perceived pitch.

% \section{The Code}

% The user has the power to adjust the frequency scale, the time length of the blips, and the duration of the sound file.

% \textbf{Frequency scale:} Frequency is set by one of the following: a frequency minimum and maximum, or a frequency maximum and a cents / y value scale value. All frequency parameters are entered inside the frequency\_args parameter, as seen below.

% \textbf{Blip length:} The time length of the blips in seconds can also be set. By default this is set to 0.5 seconds.

% \textbf{Sound file duration:} Duration is set by a total time, a duration scale (in seconds / x value), or by choosing the length of minimum or maximum time difference between datapoints. Time parameters are entered simply by defining a duration\_scale (in seconds per x value). Or alternately by passing a duration\_args dictionary with some total time, smallest delta time between points or max delta time between points, as seen in the code below.


% We advise a frequency scale set between middle C (C4) and four times the frequency of C4, a typical blip length of 0.5 seconds, and a total sound file duration between 1 and 3 seconds (longer sound files, played subsequently, are difficult for listeners to directly compare). Most humans can discern $\sim$10 cents difference in pitch. Keep this in mind when defining a your cents\_per\_value parameter value for $\frac{dc}{dy}$. Furthermore, we recommend creating a test linear sound file to be sure all your chosen parameters work well with your headphones and hearing.

% \vspace{-1.25\baselineskip}
% \begin{lstlisting}[language=Python, frame=lines,label={lst:code_direct}, caption={Example Code Setup.}, basicstyle=\footnotesize]
% from sonify import *

% C4 = 261.6 # Hz
% frequency_args = {
%   'frequency_min' : C4,
%   'frequency_max' : C4*4
%   # 'cents_per_value' : -680,
%   # 'value_min' : 0,
%   # 'value_max' : 1,
% }

% duration_args = {
%   'time_total' : 2,
%   # 'time_min' : None,
%   # 'time_max' : None,
% }
% duration_scale = None

% SN = SonifyTool(values=x, durations=y,
%                 frequency_args = frequency_args,
%                 duration_args = duration_args,
%                 # duration_scale = duration_scale,
%                 length=0.5)
% SN.SaveTone()
% \end{lstlisting}


\section{Our Astronomy Case Study}
\subsection{Citizen Science - Supernova Lightcurves}
This code was developed as part of TransientZoo, a citizen science program that will allow participants, including BVI individuals, to classify supernova lightcurves using sound. In astronomy, lightcurves depict variations in brightness of a specific astrophysical object as a function of time. The shape of these lightcurves are different depending upon the nature of the star or object creating the bright supernova explosion.


\begin{figure}[h]
\centering
\includegraphics[width=.7\linewidth]{paper/images/Ia.png}
\caption{A type Ia supernova lightcurve.}
\label{fig:Ia}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=.7\linewidth]{paper/images/IIb.png}
\caption{A type IIb supernova lightcurve.}
\label{fig:IIb}
\end{figure}

Figure \ref{fig:Ia} and \ref{fig:IIb} are two examples of successfully sonified audio light curves, for a Type IIb and Type Ia supernovae. We find that linear and plateau supernova light curves can be audibly differentiated. This approach offers a new tool for citizen science lightcurve classification.

%Note from Emily: these figures are awesome and I like the QR codes; that said, I know those are holdovers from the poster! Are QR codes still the best way to share sound files in an online-only paper? (they may well be, but I'm wondering if links or embedded mp4s or similar might be a preferred format for JOSS? It's possible they make say something about this in the documentation for the journal...)

\subsection{Other Variable Objects in Astronomy}

We’ve also explored the sonification of other time-domain data, which will eventually help TransientZoo expand into LightcurveZoo. Figures \ref{fig:EB} and \ref{fig:RRLyrae} show examples of an eclipsing binary from Kepler’s catalogue and an RR Lyrae from the author's own telescope observations. LightcurveZoo will ultimately include a collection of transients: supernovae, binaries, and variable stars.

\begin{figure}
\centering
\includegraphics[width=.7\linewidth]{paper/images/EB.png}
\caption{An eclipsing stellar binary.}
\label{fig:EB}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=.7\linewidth]{paper/images/RRLyrae.png}
\caption{A variable star, phased over its period.}
\label{fig:RRLyrae}
\end{figure}

\subsection{Acknowledgements}
Special thanks to Dr. Chris Laws and Manastash Ridge Observatory for acquisition of some of our example observations, and to Dr. Christi Miller from the Department of Speech and Hearing Sciences at the University of Washington for her consultation on the topic of human pitch perception. This work was supported by NSF grant AST 1714285 awarded to E.M.L.

\end{document}